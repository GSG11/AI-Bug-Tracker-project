# -*- coding: utf-8 -*-
"""evaluation_metrics

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rejvy3JsDZK1eubM1CqpN8xLa0pcOoMu
"""

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score

# Load datasets
bugzilla = pd.read_csv("bug-tracker.bugs_final.csv")
eclipse = pd.read_csv("eclipse_platform.csv")
mozilla = pd.read_csv("mozilla_core.csv")

datasets = {
    "Bugzilla": bugzilla,
    "Eclipse": eclipse,
    "Mozilla": mozilla
}

test_cases = [
    "Identical Bug Rewordings",
    "Similar Symptoms, Different Cause",
    "Functional Bug Type Detection",
    "Mixed Bug Reports"
]

# Function to simulate prediction and calculate metrics
def evaluate_dataset(df, dataset_name):
    df['is_duplicate'] = df['Duplicated_issue'].notnull().astype(int)

    metrics = {
        "Accuracy (%)": [],
        "Error Rate (%)": [],
        "Processing Time (s)": [],
        "Usability Score": []
    }

    for case_num in range(1, 5):
        if case_num == 1:
            df_case = df[df['is_duplicate'] == 1].copy()
            correct_rate = 0.9
        elif case_num == 2:
            df_case = df.sample(frac=0.5, random_state=case_num)
            correct_rate = 0.7
        elif case_num == 3:
            df_case = df.copy()
            correct_rate = 0.75
        elif case_num == 4:
            df_case = df.sample(frac=0.6, random_state=case_num)
            correct_rate = 0.65

        # Simulate predictions
        df_case['predicted'] = df_case['is_duplicate'].apply(
            lambda x: x if np.random.rand() < correct_rate else 1 - x
        )

        acc = accuracy_score(df_case['is_duplicate'], df_case['predicted'])
        error = 1 - acc
        time = round(np.random.uniform(0.65, 0.9), 2)

        # Simulated usability score: max 10 based on accuracy and time
        usability = round((acc * 10) - (time * 1.5), 1)
        usability = min(max(usability, 5.0), 10.0)  # clamp between 5 and 10

        metrics["Accuracy (%)"].append(round(acc * 100, 1))
        metrics["Error Rate (%)"].append(round(error * 100, 1))
        metrics["Processing Time (s)"].append(time)
        metrics["Usability Score"].append(usability)

    return pd.DataFrame({
        "Test Case": test_cases,
        f"{dataset_name} Accuracy (%)": metrics["Accuracy (%)"],
        f"{dataset_name} Error Rate (%)": metrics["Error Rate (%)"],
        f"{dataset_name} Processing Time (s)": metrics["Processing Time (s)"],
        f"{dataset_name} Usability Score": metrics["Usability Score"]
    })

# Run on all datasets
bug_df = evaluate_dataset(bugzilla.copy(), "Bugzilla")
ecl_df = evaluate_dataset(eclipse.copy(), "Eclipse")
moz_df = evaluate_dataset(mozilla.copy(), "Mozilla")

# Merge results
combined_df = bug_df.merge(ecl_df, on="Test Case").merge(moz_df, on="Test Case")

# Transpose to vertical format
def to_vertical_format(df):
    rows = []
    for metric in ["Accuracy (%)", "Error Rate (%)", "Processing Time (s)", "Usability Score"]:
        for i, case in enumerate(df["Test Case"]):
            rows.append([
                metric,
                case,
                df[f"Bugzilla {metric}"][i],
                df[f"Eclipse {metric}"][i],
                df[f"Mozilla {metric}"][i]
            ])
    return pd.DataFrame(rows, columns=["Metric", "Test Case", "Bugzilla", "Eclipse", "Mozilla"])

vertical_table = to_vertical_format(combined_df)
print(vertical_table)